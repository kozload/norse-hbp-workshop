{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "source": [
    "## Tensors and Autograd\n",
    "\n",
    "Torch at its core implements automatic differentiation on arbitrary\n",
    "computation graphs of tensors. To give a simple example consider\n",
    "addition of two tensors\n",
    "$$\n",
    "y = x_1 + x_2\n",
    "$$\n",
    "then we have\n",
    "$$\n",
    "dy = dx_1 + dx_2\n",
    "$$\n",
    "and therefore the gradient passed into $dy$ gets copied to the input\n",
    "nodes $dx_1$ and $dx_2$. Indeed this is what happens if do\n",
    "this simple operation in PyTorch:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1.]), tensor([1., 1., 1.]))"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "x_1 = torch.randn(3, requires_grad=True)\n",
    "x_2 = torch.randn(3, requires_grad=True)\n",
    "y = x_1 + x_2\n",
    "\n",
    "y.backward(torch.ones(3))\n",
    "x_1.grad, x_2.grad"
   ]
  },
  {
   "source": [
    "Similarly for multiplication\n",
    "$$ \n",
    "y = x_1 \\cdot x_2\n",
    "$$\n",
    "we have that\n",
    "$$\n",
    "dy = x_2 \\cdot dx_1  + x_1 \\cdot dx_2\n",
    "$$\n",
    "Which means that whatever gradient gets passed into $dy$\n",
    "get's multiplied by the input $x_2$ for the node $dx_1$ and\n",
    "by $x_1$ for the node $dx_2$:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([2., 3., 4.]), tensor([1., 2., 3.]))"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "x_1 = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "x_2 = torch.tensor([2.0, 3.0, 4.0], requires_grad=True)\n",
    "y = x_1 * x_2\n",
    "y.backward(torch.ones(3))\n",
    "x_1.grad, x_2.grad"
   ]
  },
  {
   "source": [
    "### Exercise:\n",
    "Change the input tensor passed into $y$ in the backward calculation. Can you predict what will happen?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Extending the Autograd Mechanism\n",
    "\n",
    "If you compose primitives provided by PyTorch it will be able to the gradient computation\n",
    "initiated by the \"backward\" call for you. This is why it is called \"Autograd\". However in\n",
    "some situations it is advantageous to implement your own autograd primitives.\n",
    "\n",
    "To give you a first example, assume that the multiplication primitive was not build into PyTorch here is how you could implement it yourself:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mult(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x_1, x_2):\n",
    "        ctx.save_for_backward(x_1, x_2)\n",
    "        return x_1 * x_2\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dy):\n",
    "        x_1, x_2 = ctx.saved_tensors\n",
    "        return (x_2 * dy, x_1 * dy)\n",
    "\n",
    "mult = Mult.apply"
   ]
  },
  {
   "source": [
    "Indeed we get the same result as above:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([2., 3., 4.]), tensor([1., 2., 3.]))"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "x_1 = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "x_2 = torch.tensor([2.0, 3.0, 4.0], requires_grad=True)\n",
    "y = mult(x_1, x_2)\n",
    "y.backward(torch.ones(3))\n",
    "x_1.grad, x_2.grad"
   ]
  },
  {
   "source": [
    "### Exercise\n",
    "Implement your own addition primitive and check that it matches with the default addition primitive provided by PyTorch."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Sequential Composition\n",
    "\n",
    "A lot of neural networks can be described as the sequential composition of layers, the autograd mechanism then ensures that errors are propagated in reverse order through all layers. Consider\n",
    "for example a function that multiplies its input by $2$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_by_2(x):\n",
    "    return 2 * x"
   ]
  },
  {
   "source": [
    "Then we can express a function that multiplies its input by 8 as follows:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_by_8(x):\n",
    "    x = mult_by_2(x)\n",
    "    x = mult_by_2(x)\n",
    "    x = mult_by_2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([8., 8., 8.])"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "x_1 = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = mult_by_8(x_1)\n",
    "y.backward(torch.ones(3))\n",
    "x_1.grad"
   ]
  },
  {
   "source": [
    "Now imagine that for some reason we were unsure what the best value to multiply by in each layer  would be, so we instead introduce parameters $p_1, p_2, p_3$ by which we want to multiply."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_by_p(x, p):\n",
    "    x = mult(x, p[0]) # we are using our \"own\" multiplication function here\n",
    "    x = mult(x, p[1])\n",
    "    x = mult(x, p[2])\n",
    "    return x"
   ]
  },
  {
   "source": [
    "The final step is then to find \"good\" parameters given a certain task. If the task\n",
    "is for example for the function to multiply by 8 overall, which it surely can\n",
    "solve using the parameters $p_1 = 2, p_2 = 2, p_3 = 2$, we can instead create training\n",
    "data and target data and use an optimisation algorithm to find a solution to\n",
    "this problem aswell."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Final loss: 0.030491871759295464\n"
     ]
    }
   ],
   "source": [
    "# we initially choose randomly chosen parameters \n",
    "p = [torch.randn(3, requires_grad=True), torch.randn(3, requires_grad=True), torch.randn(3, requires_grad=True)]\n",
    "\n",
    "# Number of training samples \n",
    "K = 1000\n",
    "\n",
    "# this would correspond to the input dataset in a real world example\n",
    "# we generate K random triples of three numbers\n",
    "xs = torch.randn(K, 3, requires_grad=False)\n",
    "\n",
    "# our target is for the data to be multiplied by 8 \n",
    "ys = 8 * xs\n",
    "\n",
    "# the loss measures how well we are currently doing, here\n",
    "# we choose to compare using the l1-norm\n",
    "loss = torch.nn.L1Loss()\n",
    "\n",
    "# the last ingredient is the optimisation algorithm to use\n",
    "# SGD performs the update step\n",
    "#\n",
    "# p[i] = p[i] + lr * p[i].grad\n",
    "#\n",
    "opt = torch.optim.SGD(p, lr=0.01)\n",
    "\n",
    "# Number of steps we want to optimize for\n",
    "N = 1000\n",
    "\n",
    "losses = []\n",
    "\n",
    "# optimization loop\n",
    "for i in range(N):\n",
    "    # set all gradients p[i].grad to zero\n",
    "    opt.zero_grad()\n",
    "    # compute the distance from the desired outcome\n",
    "    l = loss(mult_by_p(xs, p), ys)\n",
    "    # propagate the gradient\n",
    "    l.backward()\n",
    "    # take a gradient descent step \n",
    "    opt.step()\n",
    "    # save the loss for later plotting\n",
    "    losses.append(l.data)\n",
    "\n",
    "print(f\"Final loss: {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fec7afb3f70>]"
      ]
     },
     "metadata": {},
     "execution_count": 119
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 379.7 248.518125\" width=\"379.7pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-01-30T16:46:13.900908</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 379.7 248.518125 \nL 379.7 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 37.7 224.64 \nL 372.5 224.64 \nL 372.5 7.2 \nL 37.7 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mf5ea17cd77\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.918182\" xlink:href=\"#mf5ea17cd77\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(49.736932 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"113.851843\" xlink:href=\"#mf5ea17cd77\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 200 -->\n      <g transform=\"translate(104.308093 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"174.785504\" xlink:href=\"#mf5ea17cd77\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 400 -->\n      <g transform=\"translate(165.241754 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.719165\" xlink:href=\"#mf5ea17cd77\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 600 -->\n      <g transform=\"translate(226.175415 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"296.652826\" xlink:href=\"#mf5ea17cd77\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 800 -->\n      <g transform=\"translate(287.109076 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"357.586486\" xlink:href=\"#mf5ea17cd77\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1000 -->\n      <g transform=\"translate(344.861486 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m8e53a29e25\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"37.7\" xlink:href=\"#m8e53a29e25\" y=\"169.444001\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- $\\mathdefault{10^{-1}}$ -->\n      <g transform=\"translate(7.2 173.243219)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-49\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-48\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-8722\"/>\n       <use transform=\"translate(186.855469 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"37.7\" xlink:href=\"#m8e53a29e25\" y=\"84.776849\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(13.1 88.576067)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-49\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-48\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"m081b8ac75c\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"37.7\" xlink:href=\"#m081b8ac75c\" y=\"213.714655\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"37.7\" xlink:href=\"#m081b8ac75c\" y=\"203.136448\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"37.7\" xlink:href=\"#m081b8ac75c\" y=\"194.931353\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"37.7\" xlink:href=\"#m081b8ac75c\" y=\"188.227303\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"37.7\" xlink:href=\"#m081b8ac75c\" y=\"182.559108\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"37.7\" xlink:href=\"#m081b8ac75c\" y=\"177.649096\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"37.7\" xlink:href=\"#m081b8ac75c\" y=\"173.318157\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"37.7\" xlink:href=\"#m081b8ac75c\" y=\"143.956648\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"37.7\" xlink:href=\"#m081b8ac75c\" y=\"129.047503\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"37.7\" xlink:href=\"#m081b8ac75c\" y=\"118.469296\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"37.7\" xlink:href=\"#m081b8ac75c\" y=\"110.264201\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"37.7\" xlink:href=\"#m081b8ac75c\" y=\"103.56015\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"37.7\" xlink:href=\"#m081b8ac75c\" y=\"97.891956\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"37.7\" xlink:href=\"#m081b8ac75c\" y=\"92.981943\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"37.7\" xlink:href=\"#m081b8ac75c\" y=\"88.651005\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"37.7\" xlink:href=\"#m081b8ac75c\" y=\"59.289496\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_19\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"37.7\" xlink:href=\"#m081b8ac75c\" y=\"44.380351\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_20\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"37.7\" xlink:href=\"#m081b8ac75c\" y=\"33.802144\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_21\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"37.7\" xlink:href=\"#m081b8ac75c\" y=\"25.597049\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_22\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"37.7\" xlink:href=\"#m081b8ac75c\" y=\"18.892998\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_23\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"37.7\" xlink:href=\"#m081b8ac75c\" y=\"13.224804\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_24\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"37.7\" xlink:href=\"#m081b8ac75c\" y=\"8.314791\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_31\">\n    <path clip-path=\"url(#p50d6789e85)\" d=\"M 52.918182 17.083636 \nL 65.409582 18.468283 \nL 73.02629 19.528387 \nL 78.814988 20.542649 \nL 83.689681 21.61077 \nL 87.955037 22.774977 \nL 91.611057 24.008795 \nL 94.65774 25.260835 \nL 97.399754 26.6168 \nL 99.837101 28.058078 \nL 101.969779 29.552565 \nL 104.102457 31.325507 \nL 105.930467 33.127435 \nL 107.758477 35.263828 \nL 108.367813 36.066375 \nL 108.672482 35.77921 \nL 108.97715 36.163176 \nL 109.281818 35.884543 \nL 109.586486 36.262951 \nL 109.891155 35.992918 \nL 110.195823 36.365822 \nL 110.500491 36.104484 \nL 110.80516 36.471941 \nL 111.109828 36.219317 \nL 111.414496 36.581369 \nL 111.719165 36.337577 \nL 112.023833 36.694273 \nL 112.328501 36.459401 \nL 112.63317 36.81079 \nL 112.937838 36.58492 \nL 113.242506 36.931061 \nL 113.547174 36.714281 \nL 113.851843 37.055227 \nL 114.156511 36.847652 \nL 114.461179 37.183473 \nL 114.765848 36.985199 \nL 115.070516 37.315949 \nL 115.375184 37.127097 \nL 115.679853 37.45283 \nL 115.984521 37.273515 \nL 116.289189 37.594324 \nL 116.593857 37.424662 \nL 116.898526 37.740606 \nL 117.203194 37.580734 \nL 117.507862 37.8919 \nL 117.812531 37.741955 \nL 118.117199 38.048408 \nL 118.421867 37.908544 \nL 118.726536 38.210387 \nL 119.031204 38.080744 \nL 119.335872 38.378072 \nL 119.640541 38.258828 \nL 119.945209 38.551712 \nL 120.249877 38.443042 \nL 120.554545 38.731612 \nL 120.859214 38.633683 \nL 121.163882 38.918017 \nL 121.46855 38.831066 \nL 121.773219 39.111284 \nL 122.077887 39.03548 \nL 122.382555 39.311713 \nL 122.687224 39.247316 \nL 122.991892 39.519666 \nL 123.29656 39.466914 \nL 123.601229 39.735529 \nL 123.905897 39.694645 \nL 124.210565 39.959675 \nL 124.515233 39.930957 \nL 124.819902 40.19253 \nL 125.12457 40.176278 \nL 125.429238 40.434544 \nL 125.733907 40.43108 \nL 126.038575 40.686222 \nL 126.343243 40.695838 \nL 126.647912 40.948062 \nL 126.95258 40.971148 \nL 127.257248 41.220613 \nL 127.561916 41.257531 \nL 127.866585 41.504465 \nL 128.171253 41.555652 \nL 128.475921 41.800277 \nL 128.78059 41.866161 \nL 129.085258 42.108683 \nL 129.389926 42.189771 \nL 129.694595 42.430486 \nL 129.999263 42.52726 \nL 130.913268 43.117379 \nL 131.522604 43.484259 \nL 132.436609 44.033681 \nL 137.006634 47.701394 \nL 138.529975 49.338257 \nL 138.834644 49.602725 \nL 139.748649 50.783481 \nL 140.053317 51.063718 \nL 140.357985 51.572623 \nL 140.662654 51.862684 \nL 140.967322 52.411562 \nL 141.27199 52.712999 \nL 141.576658 53.305091 \nL 141.881327 53.619624 \nL 142.185995 54.258614 \nL 142.490663 54.588239 \nL 142.795332 55.278365 \nL 143.1 55.625332 \nL 143.404668 56.371442 \nL 143.709337 56.738399 \nL 144.014005 57.546113 \nL 144.318673 57.136524 \nL 144.623342 57.552856 \nL 144.92801 57.142134 \nL 145.232678 57.559668 \nL 145.537346 57.147828 \nL 145.842015 57.566552 \nL 146.146683 57.153577 \nL 146.451351 57.573509 \nL 146.75602 57.159397 \nL 147.060688 57.580538 \nL 147.365356 57.165296 \nL 147.670025 57.587627 \nL 147.974693 57.171267 \nL 148.279361 57.594805 \nL 148.584029 57.177321 \nL 148.888698 57.602035 \nL 149.193366 57.183451 \nL 149.498034 57.609367 \nL 149.802703 57.189648 \nL 150.107371 57.616767 \nL 150.412039 57.195925 \nL 150.716708 57.624265 \nL 151.021376 57.202286 \nL 151.326044 57.631823 \nL 151.630713 57.208722 \nL 151.935381 57.639467 \nL 152.240049 57.215251 \nL 152.544717 57.647187 \nL 152.849386 57.221856 \nL 153.154054 57.655002 \nL 153.458722 57.228536 \nL 153.763391 57.66289 \nL 154.068059 57.235313 \nL 154.372727 57.670867 \nL 154.677396 57.242174 \nL 154.982064 57.678955 \nL 155.286732 57.249119 \nL 155.5914 57.687104 \nL 155.896069 57.256161 \nL 156.200737 57.695346 \nL 156.505405 57.26327 \nL 156.810074 57.7037 \nL 157.114742 57.270493 \nL 157.41941 57.712136 \nL 157.724079 57.277809 \nL 158.028747 57.720674 \nL 158.333415 57.285205 \nL 158.638084 57.729298 \nL 158.942752 57.292707 \nL 159.24742 57.738021 \nL 159.552088 57.300301 \nL 159.856757 57.746855 \nL 160.161425 57.307992 \nL 160.466093 57.755771 \nL 160.770762 57.315793 \nL 161.07543 57.764808 \nL 161.380098 57.323695 \nL 161.684767 57.773934 \nL 161.989435 57.331699 \nL 162.294103 57.783185 \nL 162.598771 57.339804 \nL 162.90344 57.787313 \nL 163.208108 57.346128 \nL 163.512776 57.782542 \nL 163.817445 57.368532 \nL 164.122113 57.777889 \nL 164.426781 57.391042 \nL 164.73145 57.77335 \nL 165.036118 57.413666 \nL 165.340786 57.768904 \nL 165.645455 57.43642 \nL 165.950123 57.764606 \nL 166.254791 57.459292 \nL 166.559459 57.760401 \nL 166.864128 57.482287 \nL 167.168796 57.756339 \nL 167.473464 57.505406 \nL 167.778133 57.752396 \nL 168.082801 57.528664 \nL 168.387469 57.748579 \nL 168.692138 57.552066 \nL 168.996806 57.744888 \nL 169.301474 57.575588 \nL 169.606143 57.74132 \nL 169.910811 57.599263 \nL 170.215479 57.737882 \nL 170.520147 57.623063 \nL 170.824816 57.734579 \nL 171.129484 57.646999 \nL 171.434152 57.73142 \nL 171.738821 57.671089 \nL 172.348157 57.695325 \nL 173.56683 57.744249 \nL 174.480835 57.717696 \nL 174.785504 57.793777 \nL 175.090172 57.715378 \nL 175.39484 57.818785 \nL 175.699509 57.713223 \nL 176.004177 57.843941 \nL 176.308845 57.711224 \nL 176.613514 57.869269 \nL 176.918182 57.709394 \nL 177.22285 57.894759 \nL 177.527518 57.707685 \nL 177.832187 57.920423 \nL 178.136855 57.706169 \nL 178.441523 57.946257 \nL 178.746192 57.704796 \nL 179.05086 57.972274 \nL 179.355528 57.703591 \nL 179.660197 57.998461 \nL 179.964865 57.702562 \nL 180.269533 58.024837 \nL 180.574201 57.701693 \nL 180.87887 58.051393 \nL 181.183538 57.701017 \nL 181.488206 58.078142 \nL 181.792875 57.700514 \nL 182.097543 58.098429 \nL 182.402211 57.700274 \nL 182.70688 58.112423 \nL 183.011548 57.71311 \nL 183.316216 58.126631 \nL 183.620885 57.726147 \nL 183.925553 58.141009 \nL 184.230221 57.73934 \nL 184.534889 58.155589 \nL 184.839558 57.752749 \nL 185.144226 58.170366 \nL 185.448894 57.766347 \nL 185.753563 58.185345 \nL 186.058231 57.780156 \nL 186.362899 58.200551 \nL 186.667568 57.794156 \nL 186.972236 58.215937 \nL 187.276904 57.808371 \nL 187.581572 58.231548 \nL 187.886241 57.822824 \nL 188.190909 58.247361 \nL 188.495577 57.837464 \nL 188.800246 58.263402 \nL 189.104914 57.852341 \nL 189.409582 58.279668 \nL 189.714251 57.867435 \nL 190.018919 58.296163 \nL 190.323587 57.882755 \nL 190.628256 58.312892 \nL 190.932924 57.898309 \nL 191.237592 58.329863 \nL 191.54226 57.914106 \nL 191.846929 58.347081 \nL 192.151597 57.930116 \nL 192.456265 58.364547 \nL 192.760934 57.946409 \nL 193.065602 58.382256 \nL 193.37027 57.962924 \nL 193.674939 58.400227 \nL 193.979607 57.979709 \nL 194.284275 58.418458 \nL 194.588943 57.996756 \nL 194.893612 58.43696 \nL 195.19828 58.014056 \nL 195.502948 58.455715 \nL 195.807617 58.031644 \nL 196.112285 58.474768 \nL 196.416953 58.049498 \nL 196.721622 58.494096 \nL 197.02629 58.067633 \nL 197.330958 58.513713 \nL 197.635627 58.086061 \nL 197.940295 58.533624 \nL 198.244963 58.104787 \nL 198.549631 58.553821 \nL 198.8543 58.123794 \nL 199.158968 58.574338 \nL 199.463636 58.143104 \nL 199.768305 58.595164 \nL 200.072973 58.16273 \nL 200.377641 58.616324 \nL 200.68231 58.182686 \nL 200.986978 58.637781 \nL 201.291646 58.202946 \nL 201.596314 58.659586 \nL 201.900983 58.223533 \nL 202.205651 58.681719 \nL 202.510319 58.244459 \nL 202.814988 58.704189 \nL 203.119656 58.265743 \nL 203.424324 58.727036 \nL 203.728993 58.287371 \nL 204.033661 58.750229 \nL 204.338329 58.309354 \nL 204.642998 58.773786 \nL 204.947666 58.331712 \nL 205.252334 58.797697 \nL 205.557002 58.354431 \nL 205.861671 58.822016 \nL 206.166339 58.377527 \nL 206.471007 58.846712 \nL 206.775676 58.401018 \nL 207.080344 58.871809 \nL 207.385012 58.424901 \nL 207.689681 58.897306 \nL 207.994349 58.449173 \nL 208.299017 58.923241 \nL 208.603686 58.473872 \nL 208.908354 58.949572 \nL 209.213022 58.498986 \nL 209.51769 58.976356 \nL 209.822359 58.524543 \nL 210.127027 59.003578 \nL 210.431695 58.55053 \nL 210.736364 59.031242 \nL 211.041032 58.576969 \nL 211.3457 59.05938 \nL 211.650369 58.603862 \nL 211.955037 59.087989 \nL 212.259705 58.631219 \nL 212.564373 59.117082 \nL 212.869042 58.659074 \nL 213.17371 59.146652 \nL 213.478378 58.687402 \nL 213.783047 59.17674 \nL 214.087715 58.716245 \nL 214.392383 59.207325 \nL 214.697052 58.745585 \nL 215.00172 59.238465 \nL 215.306388 58.775464 \nL 215.611057 59.270118 \nL 215.915725 58.805859 \nL 216.220393 59.302335 \nL 216.525061 58.836826 \nL 216.82973 59.335107 \nL 217.134398 58.868338 \nL 217.439066 59.368451 \nL 217.743735 58.900437 \nL 218.048403 59.402386 \nL 218.353071 58.933111 \nL 218.65774 59.436926 \nL 218.962408 58.966396 \nL 219.267076 59.472089 \nL 219.571744 59.00029 \nL 219.876413 59.493659 \nL 220.181081 59.041503 \nL 220.485749 59.51518 \nL 220.790418 59.091398 \nL 221.095086 59.537327 \nL 221.399754 59.14198 \nL 221.704423 59.560149 \nL 222.009091 59.193275 \nL 222.313759 59.58364 \nL 222.618428 59.245294 \nL 222.923096 59.607799 \nL 223.227764 59.298058 \nL 223.532432 59.632683 \nL 223.837101 59.351569 \nL 224.141769 59.658281 \nL 224.446437 59.405844 \nL 224.751106 59.684624 \nL 225.055774 59.460937 \nL 225.360442 59.71171 \nL 225.665111 59.516836 \nL 225.969779 59.739584 \nL 226.274447 59.573582 \nL 226.579115 59.768227 \nL 226.883784 59.631155 \nL 227.188452 59.797719 \nL 227.49312 59.689633 \nL 227.797789 59.828013 \nL 228.102457 59.749005 \nL 228.407125 59.859167 \nL 228.711794 59.80931 \nL 229.32113 59.87055 \nL 230.539803 59.995977 \nL 231.453808 60.028497 \nL 231.758477 60.125491 \nL 232.063145 60.065253 \nL 232.367813 60.191857 \nL 232.672482 60.103019 \nL 232.97715 60.259334 \nL 233.281818 60.141818 \nL 233.586486 60.32793 \nL 233.891155 60.181697 \nL 234.195823 60.397711 \nL 234.500491 60.222669 \nL 234.80516 60.468682 \nL 235.109828 60.264783 \nL 235.414496 60.540887 \nL 235.719165 60.308049 \nL 236.023833 60.614357 \nL 236.328501 60.352494 \nL 236.63317 60.689128 \nL 236.937838 60.398177 \nL 237.242506 60.765254 \nL 237.547174 60.445085 \nL 237.851843 60.842736 \nL 238.156511 60.493311 \nL 238.461179 60.921642 \nL 238.765848 60.542853 \nL 239.070516 61.002014 \nL 239.375184 60.593766 \nL 239.679853 61.075724 \nL 239.984521 60.64709 \nL 240.289189 61.144852 \nL 240.593857 60.714942 \nL 240.898526 61.215509 \nL 241.203194 60.784289 \nL 241.507862 61.28774 \nL 241.812531 60.85523 \nL 242.117199 61.361589 \nL 242.421867 60.927769 \nL 242.726536 61.437102 \nL 243.031204 61.001968 \nL 243.335872 61.514343 \nL 243.640541 61.077891 \nL 243.945209 61.593348 \nL 244.249877 61.155577 \nL 244.554545 61.674196 \nL 244.859214 61.235076 \nL 245.163882 61.756896 \nL 245.46855 61.316462 \nL 245.773219 61.841544 \nL 246.077887 61.399757 \nL 246.382555 61.928181 \nL 246.687224 61.485065 \nL 246.991892 62.016871 \nL 247.29656 61.572431 \nL 247.601229 62.107686 \nL 247.905897 61.661909 \nL 248.210565 62.200684 \nL 248.515233 61.753584 \nL 248.819902 62.295952 \nL 249.12457 61.847507 \nL 249.429238 62.393546 \nL 249.733907 61.943745 \nL 250.038575 62.493555 \nL 250.343243 62.042382 \nL 250.647912 62.596057 \nL 250.95258 62.143514 \nL 251.257248 62.701113 \nL 251.561916 62.24721 \nL 251.866585 62.808834 \nL 252.171253 62.353559 \nL 252.475921 62.919288 \nL 252.78059 62.462638 \nL 253.085258 63.032588 \nL 253.389926 62.574564 \nL 253.694595 63.148814 \nL 253.999263 62.689418 \nL 254.303931 63.268075 \nL 254.6086 62.807305 \nL 254.913268 63.390488 \nL 255.217936 62.928317 \nL 255.522604 63.516135 \nL 255.827273 63.052591 \nL 256.131941 63.645161 \nL 256.436609 63.180212 \nL 256.741278 63.777669 \nL 257.045946 63.311341 \nL 257.350614 63.913801 \nL 257.655283 63.446049 \nL 257.959951 64.053667 \nL 258.264619 63.584527 \nL 258.569287 64.197428 \nL 258.873956 63.726883 \nL 259.178624 64.345203 \nL 259.483292 63.873252 \nL 259.787961 64.497175 \nL 260.092629 64.02382 \nL 260.397297 64.653473 \nL 260.701966 64.178705 \nL 261.006634 64.814299 \nL 261.311302 64.338105 \nL 261.615971 64.979804 \nL 261.920639 64.502178 \nL 262.225307 65.150178 \nL 262.529975 64.671124 \nL 262.834644 65.325611 \nL 263.139312 64.845123 \nL 263.44398 65.506303 \nL 263.748649 65.024406 \nL 264.053317 65.692475 \nL 264.357985 65.209141 \nL 264.662654 65.884364 \nL 264.967322 65.399607 \nL 265.27199 66.082205 \nL 265.576658 65.595998 \nL 265.881327 66.286232 \nL 266.185995 65.798602 \nL 266.490663 66.496725 \nL 266.795332 66.007639 \nL 267.1 66.713954 \nL 267.404668 66.223454 \nL 267.709337 66.938214 \nL 268.014005 66.44629 \nL 268.318673 67.169834 \nL 268.623342 66.676489 \nL 268.92801 67.409154 \nL 269.232678 66.914362 \nL 269.537346 67.656479 \nL 269.842015 67.160268 \nL 270.146683 67.912224 \nL 270.451351 67.41459 \nL 270.75602 68.176775 \nL 271.060688 67.677729 \nL 271.365356 68.450531 \nL 271.670025 67.950075 \nL 271.974693 68.733982 \nL 272.279361 68.232111 \nL 272.584029 69.027565 \nL 272.888698 68.524282 \nL 273.193366 69.331808 \nL 273.498034 68.827127 \nL 273.802703 69.647238 \nL 274.107371 69.141179 \nL 274.412039 69.974447 \nL 274.716708 69.467006 \nL 275.021376 70.314038 \nL 275.326044 69.805241 \nL 275.630713 70.666692 \nL 275.935381 70.156528 \nL 276.240049 71.022011 \nL 276.544717 70.522506 \nL 276.849386 71.382282 \nL 277.154054 70.922443 \nL 277.458722 71.757456 \nL 277.763391 71.338179 \nL 278.068059 72.148377 \nL 278.372727 71.770675 \nL 278.677396 72.555983 \nL 278.982064 72.220868 \nL 279.286732 72.981256 \nL 279.5914 72.689877 \nL 279.896069 73.425252 \nL 280.200737 73.178858 \nL 280.505405 73.88914 \nL 280.810074 73.689069 \nL 281.114742 74.374169 \nL 281.41941 74.221901 \nL 281.724079 74.881712 \nL 282.028747 74.778825 \nL 282.333415 75.413265 \nL 282.638084 75.3615 \nL 282.942752 75.97042 \nL 283.24742 75.97172 \nL 283.552088 76.554992 \nL 283.856757 76.611471 \nL 284.161425 77.168915 \nL 284.466093 77.282884 \nL 284.770762 77.814364 \nL 285.07543 77.9884 \nL 285.380098 78.493659 \nL 285.684767 78.730627 \nL 286.598771 79.964711 \nL 287.817445 81.606842 \nL 288.73145 83.109049 \nL 289.036118 83.451076 \nL 289.340786 84.147529 \nL 289.645455 84.460927 \nL 289.950123 85.252746 \nL 290.254791 85.536953 \nL 290.559459 86.43164 \nL 290.864128 86.68599 \nL 291.168796 87.692148 \nL 291.473464 87.91585 \nL 291.778133 89.043553 \nL 292.082801 89.235715 \nL 292.387469 90.496627 \nL 292.692138 90.656235 \nL 292.996806 92.064187 \nL 293.301474 92.189942 \nL 293.606143 93.761419 \nL 293.910811 93.851854 \nL 294.215479 95.606667 \nL 294.520147 95.659891 \nL 294.824816 97.622228 \nL 295.129484 97.635987 \nL 295.434152 99.835617 \nL 295.738821 99.807095 \nL 296.043489 102.281404 \nL 296.348157 102.207057 \nL 296.652826 105.003736 \nL 296.957494 104.87892 \nL 297.262162 108.028046 \nL 297.56683 107.885507 \nL 297.871499 111.436774 \nL 298.176167 111.344976 \nL 298.480835 115.350132 \nL 298.785504 115.323788 \nL 299.090172 119.915517 \nL 299.39484 119.976287 \nL 299.699509 125.35484 \nL 300.004177 125.536286 \nL 300.308845 132.024845 \nL 300.613514 132.383368 \nL 300.918182 140.554912 \nL 301.22285 141.196324 \nL 301.527518 152.223128 \nL 301.832187 153.384062 \nL 302.136855 170.342887 \nL 302.441523 172.756454 \nL 302.746192 210.422961 \nL 303.05086 180.975691 \nL 303.355528 210.075313 \nL 303.660197 181.14517 \nL 303.964865 209.731 \nL 304.269533 181.315312 \nL 304.574201 209.390335 \nL 304.87887 181.486116 \nL 305.183538 209.052677 \nL 305.488206 181.657996 \nL 305.792875 208.717673 \nL 306.097543 181.830798 \nL 306.402211 208.385816 \nL 306.70688 182.004089 \nL 307.011548 208.057417 \nL 307.316216 182.178238 \nL 307.620885 207.731932 \nL 307.925553 182.352812 \nL 308.230221 207.409942 \nL 308.534889 182.528332 \nL 308.839558 207.090579 \nL 309.144226 182.704654 \nL 309.448894 206.773823 \nL 309.753563 182.881834 \nL 310.058231 206.459906 \nL 310.362899 183.059785 \nL 310.667568 206.148499 \nL 310.972236 183.238728 \nL 311.276904 205.839798 \nL 311.581572 183.418286 \nL 311.886241 205.53392 \nL 312.190909 183.598802 \nL 312.495577 205.230319 \nL 312.800246 183.780261 \nL 313.104914 204.929664 \nL 313.409582 183.962595 \nL 313.714251 204.631056 \nL 314.018919 184.145838 \nL 314.323587 204.334824 \nL 314.628256 184.330126 \nL 314.932924 204.041164 \nL 315.237592 184.514864 \nL 315.54226 203.75006 \nL 315.846929 184.700792 \nL 316.151597 203.461394 \nL 316.456265 184.887351 \nL 316.760934 203.175248 \nL 317.065602 185.074762 \nL 317.37027 202.891343 \nL 317.674939 185.263183 \nL 317.979607 202.609298 \nL 318.284275 185.452664 \nL 318.588943 202.329451 \nL 318.893612 185.643139 \nL 319.19828 202.05191 \nL 319.502948 185.834404 \nL 319.807617 201.776653 \nL 320.112285 186.026502 \nL 320.416953 201.503818 \nL 320.721622 186.219665 \nL 321.02629 201.232625 \nL 321.330958 186.413614 \nL 321.635627 200.963808 \nL 321.940295 186.608814 \nL 322.244963 200.696958 \nL 322.549631 186.804763 \nL 322.8543 200.431951 \nL 323.158968 187.001753 \nL 323.463636 200.168983 \nL 323.768305 187.199758 \nL 324.072973 199.907939 \nL 324.377641 187.399007 \nL 324.68231 199.648513 \nL 324.986978 187.599067 \nL 325.291646 199.391601 \nL 325.596314 187.800116 \nL 325.900983 199.135878 \nL 326.205651 188.002644 \nL 326.510319 198.882127 \nL 326.814988 188.206048 \nL 327.119656 198.630029 \nL 327.424324 188.410643 \nL 327.728993 198.3796 \nL 328.033661 188.616288 \nL 328.338329 198.131203 \nL 328.642998 188.822961 \nL 328.947666 197.884469 \nL 329.252334 189.030841 \nL 329.557002 197.639498 \nL 329.861671 189.239682 \nL 330.166339 197.396274 \nL 330.471007 189.449796 \nL 330.775676 197.154489 \nL 331.080344 189.660908 \nL 331.385012 196.914667 \nL 331.689681 189.873393 \nL 331.994349 196.676219 \nL 332.299017 190.086706 \nL 332.603686 196.345619 \nL 332.908354 190.120738 \nL 333.213022 195.505097 \nL 333.51769 190.861386 \nL 333.822359 194.683779 \nL 334.127027 191.617197 \nL 334.431695 193.880451 \nL 334.736364 192.388495 \nL 335.041032 193.094901 \nL 335.3457 193.176208 \nL 335.650369 192.325476 \nL 335.955037 193.980967 \nL 336.259705 191.57231 \nL 336.564373 194.803564 \nL 336.869042 190.834357 \nL 337.17371 195.644518 \nL 337.478378 190.111332 \nL 337.783047 196.504977 \nL 338.087715 189.402408 \nL 338.392383 197.38563 \nL 338.697052 188.707085 \nL 339.00172 198.288118 \nL 339.306388 188.024538 \nL 339.611057 199.212641 \nL 339.915725 187.354932 \nL 340.220393 200.160954 \nL 340.525061 186.697339 \nL 340.82973 201.134294 \nL 341.134398 186.051374 \nL 341.439066 202.133796 \nL 341.743735 185.416772 \nL 342.048403 203.160719 \nL 342.353071 184.793265 \nL 342.65774 204.216387 \nL 342.962408 184.180588 \nL 343.267076 205.303316 \nL 343.571744 183.577909 \nL 343.876413 206.422669 \nL 344.181081 182.985206 \nL 344.485749 207.059628 \nL 344.790418 182.480561 \nL 345.095086 207.496977 \nL 345.399754 182.271112 \nL 345.704423 207.939872 \nL 346.009091 182.062656 \nL 346.313759 208.387851 \nL 346.618428 181.855617 \nL 346.923096 208.841442 \nL 347.227764 181.649756 \nL 347.532432 209.300383 \nL 347.837101 181.445322 \nL 348.141769 209.76476 \nL 348.446437 181.242045 \nL 348.751106 210.234607 \nL 349.055774 181.040156 \nL 349.360442 210.710847 \nL 349.665111 180.839048 \nL 349.969779 211.193093 \nL 350.274447 180.639398 \nL 350.579115 211.681519 \nL 350.883784 180.440575 \nL 351.188452 212.176721 \nL 351.49312 180.242762 \nL 351.797789 212.679163 \nL 352.102457 180.046114 \nL 352.407125 213.187767 \nL 352.711794 179.850607 \nL 353.016462 213.703494 \nL 353.32113 179.656347 \nL 353.625799 214.226068 \nL 353.930467 179.463004 \nL 354.235135 214.756364 \nL 354.539803 179.271001 \nL 354.844472 214.656542 \nL 355.14914 179.144163 \nL 355.453808 214.265156 \nL 355.758477 179.305969 \nL 356.063145 213.878328 \nL 356.367813 179.468368 \nL 356.672482 213.495429 \nL 356.97715 179.63161 \nL 357.281818 213.116665 \nL 357.281818 213.116665 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 37.7 224.64 \nL 37.7 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 372.5 224.64 \nL 372.5 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 37.7 224.64 \nL 372.5 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 37.7 7.2 \nL 372.5 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p50d6789e85\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"37.7\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.semilogy(losses)"
   ]
  },
  {
   "source": [
    "We can evaluate the function with our trained parameters and\n",
    "find that provided the loss reached a sufficiently small value\n",
    "(< 0.1 say), that the function indeed approximately multiplies\n",
    "by 8:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([8.0977, 7.9388, 8.1264], grad_fn=<MultBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 122
    }
   ],
   "source": [
    "mult_by_p(torch.ones(3), p)"
   ]
  },
  {
   "source": [
    "There is of course not one unique way of multiplying by $8$ \n",
    "given three parameters and indeed gradient descent finds \n",
    "different ways of doing so. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[tensor([1.9752, 2.4792, 1.7747], requires_grad=True),\n",
       " tensor([-1.9836, -2.0287, -2.5289], requires_grad=True),\n",
       " tensor([-2.0668, -1.5784, -1.8107], requires_grad=True)]"
      ]
     },
     "metadata": {},
     "execution_count": 123
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}